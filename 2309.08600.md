# SPARSE AUTOENCODERS FIND HIGHLY INTER- PRETABLE FEATURES IN LANGUAGE MODELS
(https://arxiv.org/pdf/2309.08600)

## First pass 


polysemanticity - neurons appear to activate in multiple, semantically distinct contexts 

prevents us from identifying concise, human-understandable explanations for what neural networks do


Hypothesised cause of polysemanticity is superposition

monosemantic - better

indirect object identification task - natural language task

This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method.

<F9><F10>


Assuming that data vectors are sparsely comprised of ground truth network features, they train an autoencoder to learn these ground truth network features.

# Possibly ToRead

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001, 2023.

Mechanistic interpretability seeks to mitigate such risks through understanding how neural networks calculate their outputs, allowing us to reverse engineer parts of their internal processes and make targeted changes to them


Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders, 2023. URL https://www.alignmentforum.org/posts/ z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-outof-superposition. Accessed: 2023-05-10.


Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.





Full reference (author, year, article title, journal, etc.)

## Purpose of study

Purpose was to resolve superposition in a language model using a scalable and unsupervised method.


## Research questions

How can one interpret the outputs that a language model makes?

## conceptual/theoretical framework used

Autoencoders 

## bodies of literature cited in lit review

Neural networks being able to predict tokens based off embeddings from other neural networks


## methodology, if stated or can be inferred
## methods (site, sample, participants or whatever it may be)
## findings
## resulting arguments
## implications
## conclusion
