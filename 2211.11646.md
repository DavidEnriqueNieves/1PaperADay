
NeRF-RPN: A general framework for object detection in NeRFs


Instead of trying to detect on images, they do 3D on 3D.

How? They gather the density at

Contributions:

 First significant attempt on introducing RPN to NeRF
for 3D objection detection and related tasks.


Octrees??

Oh, it's just a 3D quadtree.

Anchor-based methods in C\N's

3D Object Detection with multiple cameras

3D object detection can be point cloud-based or RGB-based
Usually things get turned into voxels, but now even raw point clouds are being used.

With one camera:


Crust : point cloud samples which cover only the surface of an object

First, they take a raw radiance and density vocel grid from a NerF model and produces a feature pyramid.

Then they use an RPN to operate on the feature pyramid and generate an object proposal.

Different NERF variants can have different radiance field structures, but they all can be queried with view directions and spatial locations. AKA

$o$


Spherical harmonics are a form of radiance representation?

Three backbones they use:

VGG
ResNet
Swin Transformer


They use an FPN to make up for the variation in object sizes in indoor scenes.

They also use 3D convolutions instead of 2D ones with position embedding and shifted windows.

OBB - oriented bounding boxes

rotation of bounding boxes is constrained to z-axis only which is perpendicular to the ground and aligned with gravity.


Faster R-C \N - places anchors at different locations and with different sizes


there are k anchors in total
Two 1x1 convolutional layers predict the probability p that an objevt exissta and the bounding box offset t for each anchor.


los =s is a combinato=ion of binary cross entropy loss for objectness and smooth L_1 loss.

t is the bounding box offset

FCOS is the anchor free method used for the Region Proposal Network


FCOS-based RPN predicts p, t, and a centerness score c for each voxel


trilinear interpolation???

What the hell is the bounding box offset 
$g = (gx, gy , gz , gw, gl, gh, gθ )$

2D projection loss does not improve performance?

Why did they add it?




Thus, we perform extensive cleaning
based on both the NeRF reconstruction quality and the
usability of object annotations (supp mtrl)

supp mrl???


Oh, supplementary material.


They chose the sharpest frame based on the variance of the Laplacian\ldots

What??


What does 


a maximum
resolution of 200 for the longest dimension of NeRF
sampling grids for Hypersim,

mean for a Nerf?


They use AdamW. How is this different from Adam?o


Anchor-free models are doing better somehoww\ldots

They show failure cases.

# Summary

1. Category: What type of paper is this? A measure-
ment paper? An analysis of an existing system? A
description of a research prototype?

This paper is (apparently) the first paper of its kind which attempts to use NERFS for object detection without having to render.

2. Context: Which other papers is it related to? Which
theoretical bases were used to analyze the problem?

It references region proposal networks from Fast-RC\N, it uses NERF papers, it uses Feature Pyramid Networks like in (Spatial pyramid pooling in deep convolutional networks for
visual recognition.), it uses some scene dataset papers such as Hypersim, 

References instantNGP, YOLO? For some reason

3. Correctness: Do the assumptions appear to be valid?

The assumptions behind this paper are that...

They assume the objects in the scene won't be oriented in a way that is outside of the main world z-axis rotation. 

In other words, they assume objects captured in the scene won't have any pitch or roll.

4. Contributions: What are the paper’s main contribu-
tions?

They provide a unique dataset taylor made for this task, 

They also introduce one of the first 3d object detection nets that uses Nerfs without rendering. 

5. Clarity: Is the paper well written?

It seems fairly legible to me, i dunno...

6. Datasets

Hypersim, ScanNet, Scene\N, 3D-Front



Notes on Faster-RCNN
https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection


Anchors are placed throughout the convolutional feature map. 


How they are used:

Compute prob that anchor is an object and not a background

bounding box regression for adjusting anchors to better fit object


In detail:

anchors are put into two categories:

those that overlap with an IoU > 0.5 are foreground

those that overlap with an IoU < 0.1 are background


Randomly sample to form a mini batch of size 256


Uses Smooth L1 loss 






