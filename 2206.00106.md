## Name
Mario Plays on a Manifold: Generating Functional Content in Latent Space through Differential Geometry
## Purpose of study
How can we generate content for video games that is functional?
How can we learn a representation for levels in video games such that interpolating between levels is always playable?
AKA a transformation that is closed with respect to functionality.

## Research questions
## conceptual/theoretical framework used
Variational Auto Encoders
Riemannian Geometry
## bodies of literature cited in lit review
Cites papers on level generation using AI

Also cite *Introduction to Smooth Manifolds* 
## methodology, if stated or can be inferred
created a metric using the jacobian and used the pullback metric to add to the latent space

The Jacobian was taken from the function $\text{dec}(z) = \alpha(z) \text{dec}_{\mu, \sigma}(z) + (1 - \alpha(z))\text{dec}_{\mu, 10^{5}}(z)$ such that areas close to playable levels have ha higher variance and thus a higher metric value. Based off that, 

Decoded a coarse grid in latent space and applied weighting to decoder to be
able to identify which parts of the latent space were unplayable.

## methods (site, sample, participants or whatever it may be)
## findings
The diversity of the levels 

## resulting arguments
## implications
## conclusion


NOTES:

Their main use of differential geometry is in the decoder step for the Variational Autoencoder.

Their method consists of:
>(i) train a Variational Autoencoder (VAE) on tile-based data (using a low-dimensional latent space), (ii) explore its latent space for functional content using grid searches, and (iii) use this knowledge to build a discrete graph that connects only the functional content.
However, what do they want to use this for again?
I guess it's for having a graph of all possible functional content.


Essentially, they want to train a VAE such that it learns the geodesic within the space of playable games. 

Reading about Variational Auto Encoders (https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)

Autoencoders in general are types of unsupervised neural networks that can represent data by encoding it to be efficient. It consists of two parts, the encoder, and the decoder. 
The purpose of the encoder is to learn the function that best represents the data in a lower dimensionality. The decoder has the opposite job.

variational inference -

Note that Baye's theorem can be expressed in these different ways:

$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)} = \frac{P(A,B)}{P(B)}
$$ 

$$
P(A|B) = \frac{P(B|A)P(A)}{\sum_{i=1}^{n} P(B, A_i)} = \frac{P(B|A)P(A)}{\sum_{i=1}^{n} P(B|A)P(A_i)}
$$ 

Two main methods of approximation: sampling and variational inference.


In a variational autoencoder, cna describe latent attributes in proabbilistic terms.
https://www.jeremyjordan.me/variational-autoencoders/
READ this too!
https://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb

READ this too!
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence

In this work, training code means "latent variable"

Posterior distribution for the parameters of a statistical model refers to $P(\theta | Y)$, like in this picture:

![](https://miro.medium.com/v2/resize:fit:927/1*szv6Z9H6vZYZ2PdXDpRCPw.png)

Posterior density $q_\phi(z | x)$ is parametrized by a neural network (the encoder)

The conditional $p_\theta(x|z)$ is in turn modelled by the decoder.

In a hierarchical variational autoencoder, the variables are organized into a hierarchy. 

VAE - name comes from variational inference in statistics


Used an A* agent to perform simulations. 

Quantity $J^{\intercal} J$ is the pullback on the latent space, performed by "pulling back" the Eucledian metric on the data space.

Method for increasing cost of being unplayable was also replicated in N.S. Detlefsen et al.




TOREAD:
G. Arvanitidis, L. K. Hansen, and S. Hauberg, “Latent space oddity: on the curvature of deep generative models,” in International Conference on Learning Representations (ICLR), 2018.

A. Tosi, S. Hauberg, A. Vellido, and N. D. Lawrence, “Metrics for probabilistic geometries,” in The Conference on Uncertainty in Artificial Intelligence (UAI), Quebec, Canada, Jul. 2014.

VAE Paper
D. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.

N. S. Detlefsen, S. Hauberg, and W. Boomsma, “What is a meaningful representation of protein sequences?” 2020.

Understand how A* algorithm works!

H. Beik-Mohammadi, S. Hauberg, G. Arvanitidis, G. Neumann, and L. Rozo, “Learning riemannian manifolds for geodesic motion skills,” in Robotics: Science and Systems (RSS), 2021.


N. Jaquier, L. Rozo, D. G. Caldwell, and S. Calinon, “Geometry-aware manipulability learning, tracking and transfer,” vol. 20, no. 2-3, pp. 624650, 2020.

D. Schwalbe-Koda and R. G ́ omez-Bombarelli, “Generative models for automatic chemical design,” arXiv:1907.01632 [physics, stat], vol. 968, p. 445–467, 2020, arXiv: 1907.01632.


